# infomax-sbdr

TODO:

- show that we have SBDR behavior in the output generated by the trained models (like show kNN or LinearSVM classification is robust to noise? like between L1 and ours?)
- check capability for outlier detection? (like plotting accuracy vs distance/mi from the dataset, or something like that)

## To comment in meetings

- AND similarity + L1 norm have much worse utilization of units, but probably better classification (because we directly train them to be linerarly separable). Test with noise?
- SHould we test ith autoencoder? On ImageNet would be not sane to use an autoencoder, should the test be standardized and same for all dataset?

## Training checklist

- l1 norm
- reconstruction loss
- FLO function (with or without exponential)
- weight decay value of adamw (0.00001)
- similarity function

## Validation checklist

- binarization

## Plots

Use error bars in all plots

Concentration:
x: different concentration
multiple traces: different average number of non-zero units
use sharp activations

Sharpness: AND vs Jaccard (with some fixed amount)
x: different sharpness
multiple traces: different average number of non-zero units
use high concentration

Ideal case:
x: average number of non-zero units
only two traces, ideal and not
use same color but different patterns for ideal vs non-ideal

Resistance to noise
x: amount of noise
multiple traces: different average of non-zero units
use high concentration
use sharp activation

## Commands

```bash
/home/iori/py_venvs/sbdrenv/bin/python /home/iori/research/infomax-sbdr/scripts/sparse-infomax/fashionmnist/train_fashionmnist_contrastive_flonoexp.py --model dense_sigmoid_logand --number 1 --cuda_devices 2
```

```bash
/home/iori/py_venvs/sbdrenv/bin/python /home/iori/research/infomax-sbdr/scripts/sparse-infomax/cifar10/train_cifar10_contrastive_flonoexp.py --model=vgg_gavg_sigmoid_logand --number=1 --cuda_devices=3
```

```bash
/home/iori/py_venvs/sbdrenv/bin/python /home/iori/research/infomax-sbdr/scripts/sparse-infomax/cifar10/train_cifar10_contrastive_floexp_L1.py --model=vgg_gavg_sigmoid_and --number=2 --cuda_devices=2
```

```bash
/home/fra/PyEnvs/hhamster/bin/python /home/fra/Documents/PhD/Research/infomax-sbdr/scripts/sparse-infomax/fashionmnist/train_fashionmnist_contrastive_floexp_L1.py --model dense_sigmoid_and --number 1
```

## Altre cose

I want you to help me to implement a custom flax.linen architecture, similar to the custom modules AntiHebbianBase and AntiHebbianModule defined in the attached code, in the file src/infomax_sbdr/antihebbian_modules.py
The new version that I want to implement is a different variant of architecture. Similarly to the current implementation of the above modules, it should consist of a base class implementing the update rules and parameter initialization, and of a child class defining the forward pass.
I want your help to write code implementing the desired modules, placing it into a python file.
However, before writing the code, you may ask clarifying questions if you are uncertain about something, in order to help me better.
Furthermore, before writing the code I want you also to mathematically and conceptually analyze the proposed novel architecture, to help me check if the learning objective and proposed implementation would make sense.
Following, you can find a description of how the new version should work:
1. It should consider a recurrent application; the __call__ function will implement a single step, taking as input a new data sample and recurrent carry values; another method of the child class will implement a recurrent scan of input of shape (*batch_dimensions, time_steps, features), applying the module iteratively using jax.lax.can over the time axis of the batches of input sequences.
2. It will have two sets of weight: one is trained using an anti-Hebbian variant of k-centroids (described below), and the other is trained using Temporal Difference (TD) to learn a prediction of the future unit activity.
3. The forward pass on a single time step is as follow:
```python
def __call__(self, variables, x, u_prev, v_prev):
    # import jax.numpy as np
    # x, input: (*batch_dimensions, input_features)
    # u_prev, previous discounted sum of past activations: (*batch_dimensions, n_units)
    # v_prev, previous predicted value of discounted sum of future activation: (*batch_dimensions, n_units)
    
    # activate all units closer then the bias terms to the centroids located at the weights of each units
    kernel_f = variables["params"]["w_f"]["kernel"]
    bias_f = variables["params"]["w_f"]["bias"]
    y_f = np.linalg.norm(kernel_f - x[..., None], axis=-2)
    y_f = y_f - bias_f < 0
    
    # compute predicted values of future activation (as a linear regression from the discounted sum of past activations, u)
    kernel_p = variables["params"]["w_p"]["kernel"] # shape (n_units, n_units)
    bias_p = variables["params"]["w_p"]["bias"] # shape (n_units,)
    v = (u[..., None] * kernel_p).sum(axis=-2)
    # compute predicted activation as difference between previous and current prediction of future discounted activation, like in bellman equation 
    y_p = v_prev - self.gamma * v
    # threshold for binary activation
    y_p = y_p > bias_p
    
    # compute the final unit activation as AND between the forward activation and the prediction activation
    z = np.logical_and(y, z_pred_activation)

    # update discounted sum of past activations
    u = self.gamma * u_prev + z
    
    # compute TD error
    td_pred_err = v_prev - (z + self.gamma * v)
    
    
    return {
        "x": x,
        "y_f": y_f,
        "y_p": y_p,
        "z": z,
        "u": u,
        "v": v,
        "td_pred_err": td_pred_err,
    }

```
where we compute two activations, $y_f$ and $y_p$ as the activation generated by the input and by a temporal difference prediction. The forward activation is computed considering each unit as defining a centroid, and activating if the input is closer than some value (bias term) in euclidean distance. The prediction activation is computed as the temporal difference between two "value" prediction.
Note, as the activation are binary it should make sense to use them as target for value prediction, similarly to a reward.
4. The model will receive as initialization parameter also the discount factor gamma, which will be used both to update the discounted sum of past activations and the TD error. In this first implementation, the model should consider dropout.
5. The forward weights should be updated according to the value of a running average of units co-activation (implemented similarly to the original modules) and to the outputs of a forward pass. The centroids should be driven closer or further from the samples that activated them, depending if the avergae unit activity is above or below the target squared average, similarly to the original modules. The bias term should also be updated likewise, increasing it proportionally, as in the current modules for the forward weights. Differently from the original anti-hebbian architecture, where a unit decorrelation objective was pursued by a set of lateral weights, in this new version I would like the forward weights to also perform this objective, if it make sense. Indeed, I was thinking of exploiting the geometry of centroids, along, to push further from each other the centroids of two units whose co-activity moving average is above the target, and viceversa.
6. The prediction weights should be updated by considering that they are a linear approximation of the expected discounted sum of future activations, using the TD error. The linear approximation takes as input the backward (i.e., towards the past) discounted sum of past activations, that for sparse binary activations should contain a compressed version of the past recent history (and also have terms on a similar scale to the future value, as hey both use a discount factor of gamma).
The bias term, which acts as a threshold for the one step TD prediction, should be updated similarly to the one for forward weights, i.e., depending on whether the unit average firing is above or below the target.
7. The idea is that this network architecture, similarly to the original anti-hebbian modules, should create a network (recurrent, in this case) that produces sparse unit activity, and that implement some form of predictive coding through the TD learning of future activations. My hope is also that the use of TD learning should produce stable long-term dynamic of the network,making it robust to one-step prediction errors.

Now, proceed with the task as detailed at the start of this conversation.


with $W_f$ the forward weights (implementing k-centroids, activating a unit if the input lies closer then the bias to the stored weight), $W_p$ are the weight performing a prediction using a temporal difference, and the output $z$ is an AND between the activation generated by the k-centroids (forward weights) and the prediction performed by the linear TD model; finally, the recurrent input vector $v$ is a decaying trace of past activations, updated iteratively as a geometric serie

defining its own parameter update rules according to the output of a forward pass and some running statistics, that again uses threshold activation


##

Forward activation formula: In your pseudocode, you have y_f = y-f - bias_f < 0. Should this be y_f = (distance < bias_f) where distance is the norm? There seems to be a typo with "y-f".
Centroid decorrelation: For pushing centroids apart based on co-activation, what update rule did you have in mind? For example:

Push Wf,iW_{f,i}
Wf,i​ away from Wf,jW_{f,j}
Wf,j​ proportional to (Wf,j−Wf,i)(W_{f,j} - W_{f,i})
(Wf,j​−Wf,i​)?

Or something else?


Initial behavior: Given that z=yf∧ypz = y_f \land y_p
z=yf​∧yp​, and WpW_p
Wp​ starts random, won't almost no units activate initially? Should we have a warmup phase or initialize WpW_p
Wp​ specially?

Variable naming: In the pseudocode, you use both u and u_prev. Should u in the forward pass be called u_next for clarity?
Bias update for bpb_p
bp​: You mentioned updating it "similarly to forward weights" based on mean activity. Should it be: Δbp∝(ptarget−μii)\Delta b_p \propto (p_{target} - \mu_{ii})
Δbp​∝(ptarget​−μii​)?

Dropout: You mentioned "this first implementation should consider dropout" - did you mean it should NOT consider dropout, or that it should include it?
Return values: Should the scan method return all the internal activations (y_f, y_p, v, u, etc.) or just the final outputs?

Below are the answers to the clarifyig questions. If you have other doubts or matter that should be discussed in light of the answers, consider them. Afterward, I will ask you to proceed with the implementation.

Clarifying answers:

1. Forward activation formula:
yes, sorry, in my initial code I made a typo, I meant to write
```python
y_f = np.linalg.norm(kernel_f - x[..., None], axis=-2)
y_f = y_f - bias_f < 0
```
which is equivalent to `y_f = (distance < bias_f)`
2. Centroid decorrelation:
yes, I was thinking of pushing/pulling the two centroids apart proportionally to $(w_{f,j} - w_{f,i})$ and also to $p_{target}^2 - \mu_{ij}$ (which means they may also be pulled together, if their co-activation statistics is below target)
3. Initial behavior:
I would expect that, if the bias terms are initialized to 0, the weights are random, the initial value of $u_{prev}$ is 0, and the initial $v_{prev}$ is small random values, the prediction y_p would have random activation, not all 0. However, for the centroids you are right; if the `bias_f` is initialized to 0, all initial forward activations `y_f` would be 0. I am considering whether we should use a "standard" dense layer with threshold activation, similarly to the forward weights in the original version of my anti-hebbian module, implemented in the attached code as described prevoiusly. In this case, would I still be able to use some hebbian/anti-hebbian update rule that tries to produce sparse decorrelated output activity, by using only this single set of forward weights? By this I mean pushing co-activation statistics to match a target $p_{des}^2$ that considers pair-wise independent binary variables, each wih average individual activation $p_{des}$ I suppose the AND with the prediction activity `y_p` may help in achieving sparse or decorrelated activity.
4. Variable naming: I would keep `u` named as such, as it's implicit that is the value produced in the time step after `u_prev`
5. Bias update: yes, I meant something similar to the implementation of the original version, where $\Delta b_p \propto (p_{target} - \mu_{ii})$
6. Dropout: yes, apologies for the error, I meant that this first version should not consider dropout
7. Return values: the scan method should return also all the internal activations, as they will be used by the update rules (final updates being averaged over all elements in the batch dimensions and time steps, similarly to the original version)


##

Yes, let's stick with the centroid-based layer; the self-activity and the decorrelation terms that you specified matche my intentions, and the initialization strategy sounds good (in the specific case, my inputs all have components in $[0,1]$).
Now, proceed to implement the custom flax.linen architecture, similar to the custom modules AntiHebbianBase and AntiHebbianModule defined in the attached code, in the file src/infomax_sbdr/antihebbian_modules.py.
The new version should work how we have discussed so far. Write the code in a python file.

