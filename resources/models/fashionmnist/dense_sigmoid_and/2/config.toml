[model]
type = "DenseFLO"
seed = 45
activation = "mish"
out_activation = "sigmoid"
[model.kwargs]
out_features = 256
hid_features = [256, 128]
hid_features_negpmi = [128]
use_batchnorm = true
use_dropout = true
dropout_rate = 0.2
# training = true
   
[dataset]
[dataset.transform.normalization]
mean = [0.2860]
std = [0.3530]
[dataset.transform.resized_crop]
size = 28
scale = [0.9, 1.0]
ratio = [0.99, 1.01]
[dataset.transform.flip]
p = 0.5
[dataset.transform.grayscale]
p = 0.1
[dataset.transform.color_jitter]
brightness = 0.15
contrast = 0.15
saturation = 0.15
hue = 0.15

[training]
epochs = 151
batch_size = 128

[training.checkpoint]
save = true
save_interval = 5 # epochs
max_to_keep = 12

[training.dataloader]
shuffle = true
# drop the last batch to avoid recompiling jitted functions because of changing input dims
# see here https://flax-linen.readthedocs.io/en/latest/guides/data_preprocessing/full_eval.html
drop_last = true

[training.loss.l1_norm]
scale = 2.0
[training.loss.sim_fn]
type = "and"
[training.loss.sim_fn.kwargs]
# eps = 1.0e-2

[training.optimizer]
type = "adamw"
[training.optimizer.kwargs]
learning_rate = 0.001
weight_decay = 0.00001

[validation]
split = 0.16
eval_interval = 12
[validation.dataloader]
batch_size = 256
[validation.sim_fn]
type = "log_and"
quantile = 0.9
[validation.sim_fn.kwargs]
eps = 1.0e-2


