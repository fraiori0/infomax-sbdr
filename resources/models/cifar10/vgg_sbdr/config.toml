[model]
type = "VGGFLO"
seed = 68
activation = "relu"
out_activation = "sigmoid"
[model.kwargs]
out_features = 256
kernel_features = [17, 64, 128]
use_batchnorm = true
use_dropout = true
dropout_rates = [0.2, 0.3, 0.3]
training = true
   

[dataset]
flatten = false
[dataset.transform]
type="offsetscale"
[dataset.transform.kwargs]
offset = [0.4914, 0.4822, 0.4465]
scale = [0.2470, 0.2435, 0.2616]

[training]
epochs = 1000
batch_size = 64
shuffle = true
# drop the last batch to avoid recompoiling jitted functions because of changing input dims
# see here https://flax-linen.readthedocs.io/en/latest/guides/data_preprocessing/full_eval.html
drop_last = true
save = true
save_interval = 2
log_interval = 1
[training.checkpoint]
save = false
save_interval = 50
max_to_keep = 5

[training.loss.kwargs]
weight_decay = 0.0
[training.loss.sim_fn]
type = "jaccard"
[training.loss.sim_fn.kwargs]
eps = 1.0e0

[training.optimizer]
type = "adam"
[training.optimizer.kwargs]
learning_rate = 0.001
