[model]
type = "SpikeEligibilityModule"
seed = 42
[model.kwargs]
n_units = [128, 96]
p_target = [0.05, 0.05]
gamma_f = [0.6, 0.9]
gamma_l = [0.6, 0.9]
ema_momentum = [0.9, 0.95]
[model.seq]
skip_first = 4

[dataset.kwargs]
num_steps = 40
max_rate = 0.3

[training]
epochs = 100
batch_size = 128
[training.loss.sim_fn]
type = "log_and"
[training.loss.sim_fn.kwargs]
eps = 1.0e-1
[training.optimizer]
type = "adamw"
[training.optimizer.kwargs]
learning_rate = 1e-2
weight_decay = 1.0e-4


[training.checkpoint]
save = true
save_interval = 5 # epochs
max_to_keep = 12

[training.dataloader]
shuffle = true
# drop the last batch to avoid recompiling jitted functions because of changing input dims
# see here https://flax-linen.readthedocs.io/en/latest/guides/data_preprocessing/full_eval.html
drop_last = true

[validation]
split = 0.16
eval_interval = 12
[validation.dataloader]
batch_size = 256
[validation.sim_fn]
type = "log_and"
quantile = 0.9
[validation.sim_fn.kwargs]
eps = 1.0e-2


