[model]
type = "AntiHebbianTDModule"
seed = 42
[model.kwargs]
n_features = 1024
p_target = 0.05
gamma_f = 0.8
gamma_l = 0.8
momentum = 0.96
n_input_features = 784 # Poisson spikes FashionMNIST
init_variance_w_forward = 0.1
init_variance_w_prediction = 0.1
[model.seq]
skip_first = 4
   
[dataset.kwargs]
num_steps = 50
max_rate = 0.5

[training]
epochs = 100
batch_size = 64
learning_rate = 0.003

[training.checkpoint]
save = true
save_interval = 2 # epochs
max_to_keep = 12

[training.dataloader]
shuffle = true
# drop the last batch to avoid recompiling jitted functions because of changing input dims
# see here https://flax-linen.readthedocs.io/en/latest/guides/data_preprocessing/full_eval.html
drop_last = true

[validation]
split = 0.16
eval_interval = 12
[validation.dataloader]
batch_size = 256
[validation.sim_fn]
type = "log_and"
quantile = 0.9
[validation.sim_fn.kwargs]
eps = 1.0e-2


